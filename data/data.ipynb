{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58392f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Merged dataset saved as merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Load the data\n",
    "stats_df = pd.read_csv('data.csv')\n",
    "\n",
    "with open('processed_interviews.json', 'r', encoding='utf-8') as f:\n",
    "    interviews = json.load(f)\n",
    "\n",
    "# 2. Preprocess dates\n",
    "stats_df['date'] = pd.to_datetime(stats_df['Date']).dt.date  # Assuming 'Date' column exists\n",
    "\n",
    "# Extract interviews into a DataFrame\n",
    "interview_data = []\n",
    "for entry in interviews:\n",
    "    interview_data.append({\n",
    "        'date': pd.to_datetime(entry['date']).date(),\n",
    "        'title': entry['title'],\n",
    "        'url': entry['url'],\n",
    "        'qa': entry['qa']\n",
    "    })\n",
    "interviews_df = pd.DataFrame(interview_data)\n",
    "\n",
    "# 3. Merge stats and interviews based on date\n",
    "merged_df = stats_df.merge(interviews_df, how='left', on='date')\n",
    "\n",
    "# 4. Save or view merged data\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "print(\"Merged dataset saved as merged_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57047a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Load the data\n",
    "stats_df = pd.read_csv('data.csv')\n",
    "\n",
    "with open('processed_interviews.json', 'r', encoding='utf-8') as f:\n",
    "    interviews = json.load(f)\n",
    "\n",
    "# 2. Preprocess dates\n",
    "stats_df['date'] = pd.to_datetime(stats_df['Date'], errors='coerce').dt.date  # Handle invalid dates\n",
    "\n",
    "# Only keep the necessary game stats columns\n",
    "stats_df = stats_df[['date', 'Result', 'MP', 'PTS', 'FG%', '3P%', 'FT%', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF']]\n",
    "\n",
    "# Extract interviews into a DataFrame\n",
    "interview_data = []\n",
    "for entry in interviews:\n",
    "    interview_data.append({\n",
    "        'date': pd.to_datetime(entry['date'], errors='coerce').date(),\n",
    "        'questions': list(entry['qa']['questions'].values()),\n",
    "        'answers': list(entry['qa']['answers'].values())\n",
    "    })\n",
    "interviews_df = pd.DataFrame(interview_data)\n",
    "\n",
    "# 3. Merge stats and interviews based on date\n",
    "merged_df = stats_df.merge(interviews_df, how='left', on='date')\n",
    "\n",
    "# 4. Format and save as JSON\n",
    "merged_data = []\n",
    "for _, row in merged_df.iterrows():\n",
    "    if pd.isna(row['date']):\n",
    "        continue\n",
    "    questions = row['questions'] if isinstance(row['questions'], list) else []\n",
    "    answers = row['answers'] if isinstance(row['answers'], list) else []\n",
    "    merged_entry = {\n",
    "        'date': row['date'].isoformat(),\n",
    "        'game_stats': {\n",
    "            'Result': row['Result'],\n",
    "            'MP': row['MP'],\n",
    "            'PTS': row['PTS'],\n",
    "            'FG%': row['FG%'],\n",
    "            '3P%': row['3P%'],\n",
    "            'FT%': row['FT%'],\n",
    "            'TRB': row['TRB'],\n",
    "            'AST': row['AST'],\n",
    "            'STL': row['STL'],\n",
    "            'BLK': row['BLK'],\n",
    "            'TOV': row['TOV'],\n",
    "            'PF': row['PF']\n",
    "        },\n",
    "        'questions': questions,\n",
    "        'answers': answers\n",
    "    }\n",
    "    merged_data.append(merged_entry)\n",
    "\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Merged dataset saved as data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3d602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written cleaned data (without 3P% and FT%) to data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "INPUT_FILE  = 'data.json'\n",
    "OUTPUT_FILE = 'data.json'\n",
    "\n",
    "def remove_percentage_fields(input_path, output_path):\n",
    "    # Load the data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Remove \"3P%\" and \"FT%\" from each entry's game_stats\n",
    "    for entry in data:\n",
    "        stats = entry.get('game_stats', {})\n",
    "        stats.pop('3P%', None)\n",
    "        stats.pop('FT%', None)\n",
    "\n",
    "    # Write the cleaned data back out\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    remove_percentage_fields(INPUT_FILE, OUTPUT_FILE)\n",
    "    print(f\"Written cleaned data (without 3P% and FT%) to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c512956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered entries written to cleaned_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "INPUT_FILE = 'data.json'\n",
    "OUTPUT_FILE = 'cleaned_data.json'  # change to 'data.json' to overwrite\n",
    "\n",
    "def filter_entries(input_path, output_path):\n",
    "    # Load the full data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Keep only entries where both questions and answers are non-empty lists\n",
    "    filtered = [\n",
    "        entry for entry in data\n",
    "        if isinstance(entry.get('questions'), list)\n",
    "           and isinstance(entry.get('answers'), list)\n",
    "           and len(entry['questions']) > 0\n",
    "           and len(entry['answers']) > 0\n",
    "    ]\n",
    "\n",
    "    # Write filtered data back out\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filter_entries(INPUT_FILE, OUTPUT_FILE)\n",
    "    print(f\"Filtered entries written to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4ead5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformed dataset (20283 lines) to dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "INPUT_FILE  = 'cleaned_data.json'\n",
    "OUTPUT_FILE = 'dataset.json'\n",
    "\n",
    "def transform_dataset(input_path, output_path):\n",
    "    # Load original data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_entries = []\n",
    "    for entry in data:\n",
    "        date = entry.get(\"date\")\n",
    "        stats = entry.get(\"game_stats\", {})\n",
    "\n",
    "        # Extract just 'W' or 'L' from the full result string\n",
    "        full_result = stats.get(\"Result\", \"\").strip()\n",
    "        wl = full_result.split()[0] if full_result else \"\"\n",
    "\n",
    "        # Rebuild game_stats with normalized Result\n",
    "        cleaned_stats = {k: v for k, v in stats.items() if k != \"Result\"}\n",
    "        cleaned_stats[\"Result\"] = wl\n",
    "\n",
    "        # Split out each question/answer pair\n",
    "        questions = entry.get(\"questions\", [])\n",
    "        answers   = entry.get(\"answers\",   [])\n",
    "        for q, a in zip(questions, answers):\n",
    "            new_entries.append({\n",
    "                \"date\":       date,\n",
    "                \"game_stats\": cleaned_stats,\n",
    "                \"question\":   q,\n",
    "                \"answer\":     a\n",
    "            })\n",
    "\n",
    "    # Write the new flat dataset\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_entries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transform_dataset(INPUT_FILE, OUTPUT_FILE)\n",
    "    print(f\"Saved transformed dataset ({len(open(OUTPUT_FILE).read().splitlines())} lines) to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7c12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1193 rows to dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "INPUT_JSON  = 'dataset.json'\n",
    "OUTPUT_CSV   = 'dataset.csv'\n",
    "\n",
    "def json_to_csv(input_path, output_path):\n",
    "    # Load the JSON data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data found in\", input_path)\n",
    "        return\n",
    "\n",
    "    # Determine all the columns:\n",
    "    # top-level fields: date, question, answer\n",
    "    # plus all keys inside game_stats\n",
    "    sample = data[0]\n",
    "    game_stats_keys = list(sample.get('game_stats', {}).keys())\n",
    "    fieldnames = ['date'] + game_stats_keys + ['question', 'answer']\n",
    "\n",
    "    # Write CSV\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for entry in data:\n",
    "            row = {}\n",
    "            row['date'] = entry.get('date', '')\n",
    "\n",
    "            # flatten game_stats\n",
    "            stats = entry.get('game_stats', {})\n",
    "            for key in game_stats_keys:\n",
    "                row[key] = stats.get(key, '')\n",
    "\n",
    "            # Q/A\n",
    "            row['question'] = entry.get('question', '').replace('\\n', ' ').strip()\n",
    "            row['answer']   = entry.get('answer',   '').replace('\\n', ' ').strip()\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Wrote {len(data)} rows to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    json_to_csv(INPUT_JSON, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51225539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
